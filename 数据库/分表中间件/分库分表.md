---
date created: 2023-03-08 19:22
---

#数据存储

业务增长到一定量级时，单表读写性能急剧下降，无法支撑业务发展。

## 垂直拆分

- 根据**业务维度**做表的划分，这个最常见的，在微服务拆分、数据模型设计时就会考虑，不做具体介绍。

## 水平拆分

- 根据**某个字段**（即分库/分表因子）应用某种策略来做拆分，达到平摊压力的目的。

- 使用分库分表后会遇到一些挑战：
  - 分布式事务。对多个数据库有事务控制，可能会引发更严重的性能问题。
  - 跨库查询、统计、排序。查询次数放大、内存合并引发 OOM。
  - 数据倾斜。热点数据未被有效拆分。

## 水平拆分策略

| 策略        | 思想                                                                                             | 优点                                     | 缺点                                 |
| ----------- | ------------------------------------------------------------------------------------------------ | ---------------------------------------- | ------------------------------------ |
| hash        | 计算分库下标公式：hash(分库因子)%分库数                                                          | 简单                                     | 数据库扩容后数据迁移困难             |
| 一致性 hash | 部分解决 hash 算法在新增、减少数据库节点时的数据迁移问题。使用一个 hash 环来管理分库因子映射范围 | 新增、删除数据库节点后，需要迁移的数据少 | 存在数据倾斜的可能                   |
| 字典表      | 全局数据具备同一种属性，以其作为分库因子实现分库                                                 | 平滑扩容，无数据迁移成本；能避免跨库事务 | 字典表容易形成瓶颈；容易出现数据倾斜 |
| 业务索引表  | 每张表根据实际情况选择分库因子做 sharding                                                        | 数据分布均匀                             | 容易出现跨库事务问题                 |

使用何种策略？有几个原则可参考：

- 数据尽可能均匀分布。数据不均匀分布的话，分库分表的效果就要打折扣，因为很快热点数据又会发生性能瓶颈。
- 运维代价可控。数据库多了，运维成本是需要考虑的。
- 尽量避免跨库事务。跨库事务既是添加了实现复杂度，又加大了锁冲突概率，可能引发更严重的性能问题。

## 技术选型

---

### 客户端分库分表

优点：

- 未引入中间件，没有运维成本
- 业务与数据库直连，未增加 RT 成本

缺点：

- 业务发展到一定阶段，业务微服务实例数变多后，数据库连接数会是瓶颈
- 问题定位、数据治理较困难

常见技术选型：

- sharding-jdbc。当当开源，现在是 apache 顶级项目，发展较成熟，目前社区仍在快速迭代。

### proxy 分库分表

优点：

- 无数据库连接数瓶颈
- 支持异构语言

缺点：

- 存在访问热点
- 数据库访问 RT 会加大

常见技术选型：

- cobar。阿里开源，mycat、TDDL 等都基于它发展而来，目前社区已不活跃，不建议选型。
- mycat。纯社区运作，目前活跃度一般，不建议选型。
- sharding-proxy。sharding-jdbc 的 proxy 版本，版本成熟度不够，谨慎选型。
- Atlas。360 开源，从 mysql proxy 发展而来
- DDM。华为云提供的云服务，从 mycat 发展而来
- DRDS。阿里云提供的云服务，基于 TDDL 演进而来。

### 服务端分库分表

nosql

mongodb、cassandra

nosql 一般是 AP 型设计，很少会追求强一致性，所以分区能力基本不是问题，但是若业务有强一致性要求，则这条路不适合。

newsql

tidb

优点：

- 新一代数据库，既有关系型数据库的 ACID 特性，又有 nosql 的水平扩展能力。目前社区活跃，值得研究。

缺点：

- 需要自行维护环境。
- 新事物，未被广泛证明。需要有尝鲜的勇气。

### 引申

数据库的 sharding 思想同样可应用在系统层面，比如：

- MPP 架构
- SET 化技术
- share-nothing 架构

## 分布式 ID 生成器

使用分库分表后，自增主键之类的东西是没法使用了，否则容易给业务挖坑。

目前主要有两种实现思路：

**redis**
使用 incr 命令，可保证数据的全局自增、原子

优点：简单

缺点：存在访问热点

**雪花算法**
该算法的思想是：为所有参与计算 ID 的进程分配单独区间，保证 ID 永不重复。

该算法不存在访问热点，只要保证各进程分配不同的区间，性能没有瓶颈。

优点：性能高，无热点

缺点：生成的 ID 不连续

**选型建议**
若追求 ID 连续性，则使用 redis，否则使用雪花算法。

## 子表路由

### 哈希取余分区

针对 redis 来说 1 亿条数据，一般是对应 1 亿个 key value，我们把他分别存储在 N 个节点，如上图 N=3，然后用户每次读写操作，根据节点 N 使用公式 hash(key)%N 计算出哈希值，用来决定数据映射到哪一个节点上

- 优点：简单粗暴，只要提前预估好数据量，然后规划好节点，例如：3 台、30 台、300 台节点，就能保证未来一段时间内的数据支撑
- 缺点：节点扩容或收缩节点的时候就麻烦了，因为每次节点有变动数据节点映射关系需要重新计算，会导致数据的重新迁移。例如原先是 3 台，后面要新增到 8 台，要把所有的历史数据按 hash(key)%8，重新洗一遍，非常麻烦

![[Pasted image 20230312133335.png]]

### 一致性哈希分区

一致性哈希算法基本原理大概需要 3 个步骤来解释：构造一致性哈希环、节点映射、路由规则

#### 构造一致性哈希环

一致性哈希算法中首先有一个哈希函数，哈希函数产生 hash 值，所有可能的哈希值构成一个哈希空间，哈希空间为[0, 2^32-1]，这本来是一个“线性”的空间，但是在算法中通过恰当逻辑控制，使其首尾相衔接，也即是 0=2^32-1，这样就构造一个逻辑上的环形空间
![[Pasted image 20230312133638.png]]

#### 节点映射

将集群中的各 IP 节点映射到环上的某一个位置  
假设有四个节点 Node A、B、C、D，经过 ip 地址的哈希函数计算（例如：hash(192.168.1.13)），它们的位置如下：

![[Pasted image 20230312133649.png]]

#### 路由规则

路由规则包括存储（setX）和取值（getX）规则
当需要存储一个`<key-value>`对时，首先计算键 key 的 hash 值：hash(key)，这个 hash 值必然对应于一致性 hash 环上的某个位置，然后沿着这个值按顺时针找到第一个节点，并将该键值对存储在该节点上
例如有 4 个存储对象 Object A、B、C、D，经过对 key 的哈希计算后，它们的位置如图
对于各个 Object，它所真正的存储位置是按顺时针找到的第一个存储节点
例如 Object A 顺时针找到的第一个节点是 Node A，所以 Node A 负责存储 Object A，Object B 存储在 Node B

![[Pasted image 20230312133710.png]]

一致性哈希如何实现容错性和扩展性？

- **容错性** 假设 Node C 节点挂掉了，Object C 的存储丢失，如果要重新把数据补回来时，Object C 就会顺时针找到的最新节点是 Node D.也就是说 Node C 挂掉了，受影响仅仅包括 Node B 到 Node C 区间的数据，并且这些数据会转移到 Node D 进行存储
- **扩展性** 假设现在数据量大了，需要增加一台节点 Node X，Node X 位置在 Node A 到 Node B 之间，那么受到影响的仅仅是 Node A 到 Node X 间的数据，重新把 A 到 X 之间的数据洗到 Node X 上即可

![[Pasted image 20230312133752.png]]

- 优点：与哈希取余分区相比，容错性和扩展性更灵活，例如 Node C 瘫痪，只影响 Node B 到 Node C 区间的数据，影响面小；再例如增加一台节点 Node X，只影响到 Node A 到 Node B 之间的数据，不会导致哈希取余全部数据重洗
- 缺点：数据倾斜不一致性.如果在分片的集群中，节点太少，并且分布不均，一致性哈希算法就会出现部分节点数据太多，部分节点数据太少。也就是说无法控制节点存储数据的分配。如图，大部分数据都在 A 上了，B 的数据比较少

![[Pasted image 20230312133812.png]]

### 哈希槽分区

由于一致性哈希分区存在数据倾斜不一致性的问题，故引入了槽的概念

#### 什么是哈希槽

哈希槽其实就是一个数组，数组[0, 1, 2, ..., 2^14-1]形成 hash slot 空间

![[Pasted image 20230312133510.png]]

#### 把哈希槽均匀分段，分配给 redis 节点

redis 节点 1，负责存储 5461 个哈希槽的数据，编号 0 号至 5460 号哈希槽
redis 节点 2，负责存储 5462 个哈希槽的数据，编号 5461 号至 10922 号哈希槽
redis 节点 3，负责存储 5461 个哈希槽的数据，编号 10923 号至 16383 号哈希槽

![[Pasted image 20230312133520.png]]

#### 计算每条数据的 slot 空间位置

将数据 key 进行哈希取值，映射已经固定大小的 hash slot 空间上
例如：可以采用 spring redis 的 API

```xml
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-redis</artifactId>
</dependency>
```

例如：我们往 redis 设置 3 条数据：

```java
redisTemplate.opsForValue.set("A", "agan1");
redisTemplate.opsForValue.set("B", "agan2");
redisTemplate.opsForValue.set("C", "agan3");
```

然后计算 key，A、B、C 的 slot 槽位置

```java
io.lettuce.core.cluster.SlotHash.getSlot("A")=6373
io.lettuce.core.cluster.SlotHash.getSlot("B")=10374
io.lettuce.core.cluster.SlotHash.getSlot("C")=14503
```

key A、B 落在 slot 空间的 5461 至 10922 区间上，并最终存储在 Node 2 上
key C 落在 slot 空间的 10923 至 16383 区间上，并最终存储在 Node 3 上

![[Pasted image 20230312133530.png]]

#### redis 哈希槽分区的特点

- 解耦数据和节点之间的关系，例如：数据的读写只要计算出槽号就可以，节点的扩容和收缩只要重新均衡分配槽区间即可；故简化了节点扩容和收缩难度
- 节点自身维护槽的映射关系，不需要客户端（spring）或者代理服务维护槽分区和数据
- 支持节点、槽、键之间的映射查询，用于数据路由、在线伸缩等场景

![[Pasted image 20230312133542.png]]

数据和槽号是绑在一起的，spring 通过槽号找到节点
