[视频第一部分](https://www.bilibili.com/video/BV1UA411P7iA)​

[视频第二部分](https://www.bilibili.com/video/BV1rU4y17769)​

## Requirement Analysis​

### Functional Requirement​

1. 添加好友 ​
2. 聊天会话列表 ​
3. 单聊 用户 A 给用户 B 发消息 ​
4. 群聊 多个用户在一个聊天室内聊天 ​
5. 多设备登陆 ​
6. 消息漫游 ​
7. 消息已读，查看已读/未读列表 ​

### 约束 ​

1. DAU 10 亿 ​
2. 假设每人平均每天发 100 条消息，1000Mli\*100/86400 = 12Mli QPS​
3. 假设一条消息存储消耗 1kb，1000Mli*100* 1KB = 1P 每天 ​
4. 峰值预估 12Mli \*1.5 = 18Mli QPS​
5. 可靠性要求 5 个 9​
6. 收发消息延迟在 10ms 以下 ​
7. 消息时序一致性(发送与接收端的消息顺序一致，不重不漏)​
8. 万人群聊 ​
9. 可运维性 ​

## 可行解

​![[image.png]]

1. 客户端轮询来 IM Server 拉取消息 ​

2. A 携带 B 的 uid，请求 IM Server ，IM Server 生成一个邀请信令消息存储到 send_msg 与 recieve_msg 中，B 定期拉取消息，获得邀请信令。​

3. B 接收好友申请，回复 IM Server，IM Server 标记消息为已读，创建一个点对点会话，等待消息的拉取。​

4. A 给 B 发送消息，IM Server 收到 A 的消息后存储到 send_msg 和 recieve_msg 表，B 轮询拉取。​

5. B 如果有多个设备端，则独立进行消息拉取。​

6. 如果 B 的设备刚刚登陆，则去 IM server 获取离线数据，客户端维护一个 seq_id，IM Server 只会返回大于此 seq_id 的 msg。​

7. B 客户端上报 B 用户已经读取的消息，IM Server 更新 flag 标记为已读。​

8. B 从 IM Server 中拉取已读用户的列表，写一行 sql select to from recieve_msg where msg_id=1 and flag=1 。​

## 更优解 ​

### 高性能 ​

#### 接入层优化 ​

1. 轮询拉模式，无法满足实时性要求，消息不能及时触达用户 ​

   1. 使用 TCP 长链接 push 消息给用户 ​

2. 客户端如何与服务端建立长链接？​

   1. 客户端只能通过公网 IP，Socket TCP 编程直接连接服务端 ​
   2. 通过一个 IPConf 服务下发公网 IP 给到客户端，灵活扩展并可智能调度长连接 ​
   3. 建立长连接后 IM Server 业务层拿到 FD 与 uid 的映射 ​
   4. IPconf 通过协调服务与 IM Server 做服务发现，根据机器的负载状态进行负载均 ​

3. 长连接服务占用大量内存资源，IO 密集度高，而 IM Server 操作数据库逻辑较重整体性能受限 ​

   1. 分离接入层与 IM Server 业务层，长链接负责维护状态收发消息，IM Server 负责业务 ​

4. 长连接服务为维持最优的消息通路，常需要以连接维度存储大量状态信息，用于做各种调度策略优化网络通信，同时会频繁迭代一些消息信令支持各种业务开发，这将导致由于服务的频繁迭代，对于一个有状态服务重启是缓慢的，并需要大量的重建连接影响消息收发延迟，该怎么办？​

   1. 将变与不变的业务逻辑在服务上做到物理隔离，变化的服务底层使用 DB，使其做到无状态 ​
   2. 不变的消息通路(长连接网关) 保证其不会频繁迭代 ​
   3. 长连接服务同步更新 state server, IpConf 服务读取状态信息用于旁路调度决策 ​
   4. State 通过状态变化控制长连接的断开，通过 MQ 发送 close 信令用于长连接的调度 ​
   5. 如何知道客户端 B 的长连接在哪个接入层服务器上，进行消息路由并保证消息可靠送达呢？​

| 方案 ​       | 特点                                                                                                                                                                                                                                                                                                                                                                                            | 优点 ​                                                                                                                   | 缺点                                                                                                        |
| ------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------- |
| 广播         | 1. IM server 将消息扇出式的发送给所有接入层服务 ​<br>2. 接入层服务只会处理自己持有 uid 的连接，而忽略其他 ​                                                                                                                                                                                                                                                                                     | 1. 实现简单<br>​2. 适合超大聊天室场景 ​                                                                                  | 1. 消息风暴，造成系统瘫痪<br>​2. 单聊场景过多的无效通信 ​                                                   |
| 一致性哈希 ​ | 1. ipConf 服务与 IM Server 使用同一种 hash 方式 ​<br>2. 按 uid 进行分片，IM Server 直接计算出该用户连接在哪个服务器上<br>​3. 通过统一的服务注册发现系统<br>​4. 当水平扩展接入层服务时，利用虚拟节点特性迁移长链接<br>​5. 对于迁移的用户来说，通过断线重连的方法实现 ​                                                                                                                           | 1. 计算简单，消息路由没有性能损耗 ​<br>2. 实现简单 ​                                                                     | 1. 重度依赖服务发现系统的稳定性 ​<br>2. 水平扩展时需要进行连接迁移 ​<br>3. 受一致性 hash 算法的均匀性限制 ​ |
| 路由服务层 ​ | 1. 构建一个服务，底层使用 kv 存储 uid 与接入机器之间的映射关系<br>2. 同时可根据不同的会话类型组织映射关系，群组，多设备等 kv 关系 ​<br>3. 有接入层服务在线同步的方式更新 kv<br>​4. 路由服务与 IM Server 之间维护消息队列，IM Server 随意给到消息队<br>​5. 任意路由服务消费 MQ 解析此消息的路由信息确定接入层机器位置<br>​6. 根据接入层机器标识创建 routeKey，将消息发送给此接入层机器专用队列 ​ | 1. 但可靠性高<br>​2. 通过 MQ 进行解耦，并很好的应对流量峰值<br>​3. MQ 保证消息可靠送达<br>​4. 路由服务无状态可水平扩展 ​ | 1. 实现较为复杂<br>​2. 需要独立维护路由层集群<br>​3. 服务依赖底层 KV 和 MQ 的稳定性 ​                       |

​![[image (1).png]]

#### 存储层优化 ​

1. 读写比 1:9999,超大群每人发送一条消息要 1 亿分发量，应该如何控制超大群将机器资源耗尽？​

   1. 并发控制，做到按群维度限制最大并发数，保证超大群收发消息不至于导致系统崩溃 ​

2. 限制并发数量后必然会导致消息挤压，应该如何处理收发消息延迟增大的问题？​

   1. 达到并发限制后，消息如果积压则将一段窗口内的消息合并压缩后下发 ​
   2. 使用推拉结合的方式本质上就是通过信令对消息进行了压缩，但是多了一步拉取的网络调用 ​
   3. 如果用推拉结合的模式可以在一段时间窗口内聚合对同一用户的拉取信令减少网络调用次数 ​

3. 存储系统上存在的严重的写放大(扩散)问题？如何减少写入数据量，降低存储成本？​

   1. 对超大群降级为读放大模式进行存储，消息仅同步写入收件箱 ​
   2. 群状态信息，如回执消息等写入 Hbase 等存储中 ​

4. 那么回执消息应该如何处理？如何保证万人群中的已读/未读列表与数量的一致性?​

   1. 实时流处理，对于接收者已读消息的下发要同步进行，而接收者的消息已读可以异步落库 ​
   2. 对于超大群，群状态变更服务进行降级，不再发送信令通知(可自动识别群的聊天热度)​
   3. 异步写入通过重试保证最终一致性 ​

5. 从 DB 直接取数无法满足延迟 10ms 以下，应该如何优化？​

[各存储介质的读写耗时](https://zhuanlan.zhihu.com/p/99837672)​

| 方案 ​           | 特点                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | 优点 ​                                                                                                                                                                              | 缺点 ​                                                                                                                                                                                                              |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 分级存储 ​       | 1. 按 uid 维度在 memDB 上维护可排序的 list，可根据活跃程度调节最大条数 ​<br>2. 对于超大群聊退化为读扩散模式，维护会话级别的 List 并异步缓存消息状态 ​<br>3. 按群聊的会话 ID 对消息列表进行本地 LRU 缓存，对列表的大小进行限制 ​<br>4. 使用 protobuf 进行序列化压缩处理，减少存储空 ​<br>5. 本地缓存的 LRU 使用 HotRing 算法，测算热点 ​<br>6. 使用消息队列广播消息状态的变化给到本地缓存，或者使用 Gossip 算法 ​<br>7. 限制拉取会话消息的切片范围，最多到 memDB 级别。​<br>8. 对于超过一定时间的数据压缩存储在文件系统中，提供 OLAP 查询 ​                | 1. 在线请求流量在 MemDB 终止 ​<br>2. 对请求按读取热度分级处理 ​<br>3. 对离线消息同步等 range 操作支持良好 ​<br>4. 可较为简单的支持群聊的推拉结合方案 ​                              | 1. IM Server 的本地缓存预热较慢，服务重启期间会有所延<br>2. 大量使用内存，导致运维难度提升 ​<br>3. 消息的已读/未读状态的变更要维护多个缓存之间的一致性 ​<br>4. 要关注缓存命中率，缓存穿透等问题 ​                   |
| 多元 DB 存储 ​   | 1. 使用 rocksDB 等 kv 引擎存储离线消息拉链，key 为会话 ID value 是序列化消息列表 ​<br>2. 如果消息列表过长则对列表进行分列存储，key 为会话 ID，value 为 meta 索引信息 ​<br>3. 按会话 ID+分段 seqID 作为 key，value 存储消息列表，读取时做合并 ​<br>4. 支持对一个 key 的 value append 新的字节数据，支持列表的扩展与更新 ​<br>5. 对超过一定期限的消息存储在文件系统中，用于 OLAP​<br>6. 使用消息 ID 作为 key，用户 id 组成的列表作为 value，存储消息已读/未读拉链 ​<br>7. 使用[NVMe SSD](https://zh.wikipedia.org/wiki/NVM_Express)等新硬件特性加速磁盘 IO​ | 1. 使用多种 NoSQL 数据库，利用不同 DB 的优势补长取短 ​<br>2. 基于磁盘的 KV 存储解决了存储容量问题，便于运维 ​<br>3. 图数据库的使用，使其能够有更多策略上的优化 ​                    | 1. RocksDB 是单机数据库，需要自研开发分布式代理层 ​<br>2. 磁盘 kv 毕竟读写磁盘，其拉取消息列表的性能将受到影响，但吞吐却有所提升 ​<br>3. rocksDB 需要列表存储到达上限后需要自行实现逻辑实现对消息的更新，实现复杂 ​ |
| 图数据库 ​       | 1. 字节跳动分布式图数据库存储实战 ​<br>2. ​[Dgraph: 同步复制,事务,分布式图数据库](https://hardcore.feishu.cn/docs/doccnE2vGujgtt4sID4gWLiMRbh)​​<br>3. 用图数据库存储好友关系，会话消息列表等多种拉链消息 ​<br>4. 图数据库存储用户关系，与群组关系，可以运行实时的近线 OLAP 查询快速识别热点群聊等极具价值的信息，利用这些信息的近实时的快速更新，可以快速识别热点对热点群聊进行特殊处理，比如增加消息列表存储上限，对热点群聊的会话消息进行本地缓存等，能够更加准确的预测热点提高命中率 ​                                                                | 1. 低延迟，提供丰富灵活的查询功能 ​<br>2. 可以挖掘极具价值的社交信息 ​<br>3. 存储层 DB 统一方便运维 ​                                                                               | 1. 技术生态不完善，没有大规模商用开源案例 ​<br>2. 图数据库较为新颖，运行大规模图数据库运维成本高，出错后难以快速排查 ​<br>3. 图数据库提供了更多的功能，也必然导致其性能不如纯粹的 KV 引擎 ​                         |
| 存储层代理服务 ​ | 1. 增加存储代理服务层，用来屏蔽底层存储细节，使得 IM Server 无感知 ​<br>2. 代理层基于 key 做 hash 分片，底层的 kvserver 基于一致性协议进行复制，代理层进行分片与合并 ​<br>3. 对超大群聊来说，多个请求会同时拉取一份会话的消息列表，可以在代理层做一个自旋 Cache，等待消息列表被缓存后在访问，减少下游 DB 的重复工作负载 ​                                                                                                                                                                                                                                 | 1. 业务隔离，对底层存储细节屏蔽 ​<br>2. 代理层无状态可水平扩展 ​<br>3. 可以做各种策略，优化查询 ​<br>4. 消息列表缓存在代理层，减少 IM Server 的内存负担，其重启过程系统会更加稳定 ​ | 1. 增加了一层逻辑，复杂度增大 ​                                                                                                                                                                                     |

![[image (2).png]]

### 消息时序一致性 ​

[消息时序一致性](http://www.52im.net/thread-3189-1-1.html)​

1. 消息如何不丢失？​

   1. 上行消息客户端重试 ​
   2. 下行消息服务端重试 ​
   3. 接收消息方需要回复 ACK，避免无限重试 ​

2. 如何防止重复？​

   1. 使用 UUID 去重复是否可以行？​

   2. 上行消息 ​

      1. 客户端在一个会话内生成一个消息的递增 cid，服务器存储当前会话接收到的 max cid​
      2. 如果发送的消息不是 max_cid+1，则将其丢弃 ​

   3. 下行消息 ​

      1. 服务端为每个发送的消息分配 seqid，客户的维护接收到的最大 max_seq_id​
      2. 如果发送的消息不是 max_seq_id+1，则将其丢弃 ​

3. 如何保证消息是有序的？​

   1. 按某个递增的消息 ID 排序 ​
      1. 上行消息按 cid 为每个消息分配 seqID​
      2. 下行消息客户端按 seqID 进行排序 ​

4. 如何生成递增的消息 ID？​

- [解密融云 IM 产品的聊天消息 ID 生成策略](http://www.52im.net/thread-2747-1-1.html)​
- [微信的海量 IM 聊天消息序列号生成实践(算法原理篇)](http://www.52im.net/thread-1998-1-1.html)​

| 方案 ​                                                                                                       | 特点                                                                                                                                                                                                                                                                                                                   | 优点 ​                                                                                                                                                                 | 缺点 ​                                                                   |
| ------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| 纯拉模式                                                                                                     | 不存在一致性问题，只要保证上行消息一致性即可                                                                                                                                                                                                                                                                           | 实现是最为简单                                                                                                                                                         | 实时性差 ​                                                               |
| 单调递增 ID 生成器 ​                                                                                         | 保证会话内 seqID 的‘万有一失’的单调递增性+拉模式消息补洞 ​                                                                                                                                                                                                                                                             | 整体实现简单，id 生成策略简单 ​                                                                                                                                        | 1. 双方通信次数过多，群聊性能差 ​<br>2. 依赖分布式 ID 生成系统的可靠性 ​ |
| 双 ID 链方法:​[Raft -论文导读 与 ETCD 源码解读](https://hardcore.feishu.cn/docs/doccnMRVFcMWn1zsEYBrbsDf8De) | 1. 接收方为每个会话保留 preID(上一次接收到消息的 id)​<br>2. 发送方发送当前消息的 id 以及上一次发送消息的 id(preID)​<br>3. 形成消息链条，接收方对比 preID 判断是否一致来识别是否存在消息漏洞 ​<br>4. 如果 preID 不匹配，则将接收方的 preID 返回 ​<br>5. 发送方回退到接收方的 preID 的位置，重置所有消息心跳后重新发送 ​ | 不依赖 id 生成的单调性 ​                                                                                                                                               | 1. 双方通信次数过多，群聊性能差 ​<br>2. 下行消息实现机制复杂 ​           |
| 推拉结合                                                                                                     | 1. 下行消息给客户端发送拉取信令，通知客户端根据自身 seqID 拉取消息 ​<br>2. pull 请求本身可作为上一次请求的 ACK​                                                                                                                                                                                                        | 1. 下行消息不需要 ack 机制，服务端不需要维护超时重发逻辑 ​<br>2. 可通过一次拉取所有会话消息列表，减少网络调用次数 ​<br>3. 批量拉取利于对消息进行压缩，提高带宽利用率 ​ | 上行消息难以彻底解决时序性问题 ​                                         |
| 上行消息 PreID+下行消息 推拉结合 ​                                                                           |                                                                                                                                                                                                                                                                                                                        | 1. 上行保证消息时序一致性 ​<br>2. 下行保证高吞吐 ​                                                                                                                     | 上行消息在频发发送消息的场景下容易造成网络用塞 ​                         |

​![[image (3).png]]

### 高可用 ​

#### 跨越公网的长连接如果断了怎么办？​

​![[Pasted image 20230313122846.png]]

1. 心跳保活 ​

   1. 为什么应用层还需要心跳? tcp 的心跳不好用吗？​
   2. 客户端周期性发送心跳给 IM Gateway，IM Gateway 重置内部定时器 ​
   3. 控制心跳包不要过大，通常在 0.5kb 以下 ​
   4. 心跳过长，服务端感知到已经断线的客户端的效率越低，资源利用率也就越低 ​
   5. 心跳过短，则会造成心跳潮汐，给网关造成流量压力(通过随机打撒来解决问题)​
   6. 发送心跳会经过很多运营商网络，不同地区运营商对于长链接的资源回收策略不同 GGSN​
   7. 自适应心跳包(前台状态固定心跳，后台状态自适应:[测算 NAT 淘汰时间](https://cloud.tencent.com/developer/article/1030660))​

2. 断线重连 ​

   1. 客户端由于网络原因断线(频繁切换网络，做地铁)，如何能快速稳定的重新建立长链接？​

      1. 断线后快速重连几次到原来的服务端 ​
      2. 服务端发现连接断开后不会立即删除用户的状态信息，等待一个超时时间 ​
      3. 在这段时间内，客户端快速建立新连接，服务端复用原有状态信息实现重连接 ​

   2. 服务端如果崩溃导致大规模客户端重新连接造成雪崩如何处理？​

      1. IPConf 服务通过服务发现机制快速识别服务端节点故障，进行调度 ​
      2. 客户端断线后，通过一定的随机策略打散重连请求的时间 ​
      3. Get ipconf 服务拿到新的 ip 调度列表重新调度，如果原有服务器还在其中则优先选 ​

3. 心跳风暴 ​

   1. 心跳/消息超时计时器的数量与连接数和 push 消息数成正比，如果长链接服务崩溃未送达的消息如何再次重发？​

      1. 连接建立后需要调用一次离线消息同步接口，主动拉取消息，避免网关侧的状态丢 ​
      2. 将连接的状态信息全部维护在 state server 中，与 IM Gateway 可以使用 RPC 或共享内存 ​
      3. State server 可以做一定的持久化处理，比喻使用快照&checkpoint 机制 ​
      4. 但恢复的状态只能是其中一部分，与计时器相关的状态信息由于时效性无法恢复 ​

   2. 大量的定时器占用大量内存资源并且定时任务的触发会使得整个系统卡顿造成消息超时 ​

      1. 传统的计时器采用二叉堆来实现，存取定时任务的时间复杂度为 LogN​
      2. 大量的消息收发会造成定时任务的频繁插入与删除，因此性能的瓶颈就在于数据结 ​
      3. 使用[时间轮](https://blog.csdn.net/u013256816/article/details/80697456)算法可以常量级别的进行任务的插入与删除，但定时精度有所缺失 ​

#### 弱网场景下如何保证消息可靠触达?​

1. 快链路 优化 TCP 连接 ​

   1. 限定传输数据包的大小 1400 字节，避免超过 MSS 造成 IP 数据分片
   2. 放大 TCP 拥塞控制窗口，为有线网络设计的拥塞控制算法，并不适用于无线网络 ​
   3. 调整 socket 读写缓冲区，避免数据包溢出 ​
   4. 调整 RTO 初始值设置为 3s，避免重试造成的堵塞 ​
   5. 禁用 Nagle 算法延迟算法，避免小数据包被协议栈缓存 ​

2. 调度策略优化 ​

   1. 赛马机制，多个 ip 后台测速选择最快连接线路(服务端要对请求进行识别，避免资源浪费)​
   2. 调度策略基于客户端上报数据进行计算，ipconf 分发最佳 ip 进行调度 ​
   3. 基于网络环境设置不同的超时时间，超时参数由 Ipconf 服务动态下发，加入策略计算 ​
   4. 短链退化， 当长链接断线次数太多，重连过于频繁的极端弱网环境例如地铁，则退化为轮询模式收发消息 ​

3. 协议优化 ​

   1. 弱网场景下切换[QUIC 协议](http://www.52im.net/thread-2816-1-1.html)​
   2. 精简协议包，采用自定义二进制协议 ​

#### 异地多活方案如何设计？​

​![[Pasted image 20230313123046.png]]

1. 核心思想：尽量减少广域请求 ​

2. 基于跨数据中心的[服务发现](https://deepzz.com/post/the-consul-of-discovery-and-configure-services.html)系统 ​

3. 通过 IPconf 进行旁路调度，IPConf 跨数据中心感知到 IM Gateway​

4. IPConf 基于客户端 ip 位置信息进行 IDC 的流量分片 ​

5. 基于位置策略调度最优 IDC 中的最优 IM Gateway Server​

6. 如果 IDC 不可用，IPConf 服务自动切流量到可用的 IDC 上 ​

7. 每个 IDC 存储全量数据以应对机房级故障，基于底层同步组件同步数据 ​

8. IM Server 通过跨 IDC 的 Route Server 服务发现 IM Gateway 并通过跨 IDC 的 MQ 发送消息 ​

9. Route Server 将自己 IDC 创建的 kv 对广播给其他 IDC，以便于进行路由转发 ​

10. IM Server 转发消息给到接收者所在的 IDC 的 IM Gateway 上，消息内携带此消息写入的 IDC​

11. 接收者根据 push 通知中携带的多个 IDC 信息，并行的去多个 IDC 拉取消息 ​

12. 为应对转发失败的情况，客户端可以在未收到任何 push 通知的一段时间后主动 pull 消息 ​

### 可运维性 ​

1. 如何做到良好的可观测性？​

   1. 展示能够暴露问题的指标 ​

   2. 搭建云原生架构 ​

   3. 构建指标观察平台/SLA ​

      1. 识别热点情况，top 活跃群聊/会话，热 key 等 ​
      2. 最小单元的 QPS，延迟，失败率三大指标 ​
      3. 分布式链路跟踪 ​

   4. 系统指标 ​

      1. 应用服务监控 ​
      2. 数据中心/集群监控 ​
      3. Docker 系统监控 ​
      4. 物理机系 统监控 ​

   5. 运行日志 ​

      1. 链路跟踪日志 ​
      2. 关键词检索日志 ​
      3. 异常，错误日志 ​

2. 如果减少人工运维的干预？并在紧急情况下可人工快速介入？​

   1. 接入服务网格，进行流量治理 ​
   2. 超时/重试/鉴权/限流/断流/风控 等功能平台化 ​
   3. 提供热备集群 ​

3. 部署，重启，修改，异常处理如何能影响更少的用户？​

   1. 实现长连接服务的[热重启](http://www.hitzhangjie.pro/blog/2020-08-28-go%E7%A8%8B%E5%BA%8F%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%83%AD%E9%87%8D%E5%90%AF/) | [MOSN 源码解析 - 启动流程](https://mosn.io/blog/code/mosn-startup/)|[MOSN 平滑升级原理](https://mosn.io/docs/concept/smooth-upgrade/)​

   2. 严格的代码发布流程，review 机制等等 ​

![[image (4).png]]

## 扩展问题 ​

### 安全性问题 ​

1. 端到端加密，作为 IM 系统用户的聊天数据均是私密信息，在现有的协议上对用户的聊天内容进行加密存储，并在另一端用户的客户端进行解密，是一种最理想的状态。​

### 多媒体消息 ​

1. 图片/视频/媒体上传 支持断点续传等功能，定义上传信令，通过局部敏感哈希识别重复数据。

2. 表情包管理 对表情包进行编码，信令交换编码 ​

## 总结 ​

​![[Pasted image 20230313123242.png]]

## 参考资料 ​

1. [腾讯 QQ1.4 亿在线用户的技术挑战和架构演进之路](http://www.52im.net/thread-158-1-1.html)​

2. [融云技术分享：全面揭秘亿级 IM 消息的可靠投递机制](http://www.52im.net/thread-3638-1-1.html)​

3. [企业微信的 IM 架构设计揭秘:消息模型、万人群、已读回执、消息撤回](http://www.52im.net/thread-3631-1-1.html)​

4. [消息表设计](https://www.codedump.info/post/20190608-im-msg-storage/)​

5. [现代 IM 系统中的消息系统架构——架构篇-InfoQ](https://www.infoq.cn/article/yPB3Y2lv-DsFtRr5Cguv)​

6. [如何设计一个亿级消息量的 IM 系统](https://xie.infoq.cn/article/19e95a78e2f5389588debfb1c)​

7. [阿里钉钉技术分享:企业级 IM 王者--钉钉在后端架构上的过人之处](http://www.52im.net/thread-2848-1-1.html)​

8. [IM 架构优化策略汇总](https://cloud.tencent.com/developer/article/1553204)​

9. [移动端 IM 开发者必读(二):史上最全移动弱网络优化方法总结](http://www.52im.net/thread-1588-1-1.html)​

10. [弱网优化方法](https://cloud.tencent.com/developer/article/1005365)​

11. [融云首席架构师李淼:即时通讯服务异地双活的那些事儿](http://www.52im.net/article-98-1.html)​

12. [淘宝技术分享:手淘亿级移动端接入层网关的技术演进之路](http://www.52im.net/thread-3110-1-1.html)​

13. [谷歌 SRE 与运维工作的思考](https://blog.csdn.net/u013256816/article/details/104177489)
