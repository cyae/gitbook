[视频第一部分](https://www.bilibili.com/video/BV1UA411P7iA)​

[视频第二部分](https://www.bilibili.com/video/BV1rU4y17769)​

​​

## Requirement Analysis​

### Functional Requirement​

1.  添加好友​

2.  聊天会话列表​

3.  单聊 用户A给用户B发消息​

4.  群聊 多个用户在一个聊天室内聊天​

5.  多设备登陆​

6.  消息漫游​

7.  消息已读，查看已读/未读列表​

### 约束​

1.  DAU 10亿​

1.  假设每人平均每天发100条消息，1000Mli*100/86400 = 12Mli QPS​

2.  假设一条消息存储消耗1kb，1000Mli*100 * 1KB = 1P 每天​

3.  峰值预估12Mli *1.5 = 18Mli QPS​

2.  可靠性要求5个9​

3.  收发消息延迟在10ms以下​

4.  消息时序一致性(发送与接收端的消息顺序一致，不重不漏)​

5.  万人群聊​

6.  可运维性​

## 可行解​

​![[image.png]]

![](blob:https://bytedancecampus1.feishu.cn/c6a913a6-1fd6-4034-8752-e15642093f0a)

1.  客户端轮询来IM Server拉取消息​

2.  A携带B的uid，请求IM Server ，IM Server生成一个邀请信令消息存储到 send_msg与recieve_msg中，B定期拉取消息，获得邀请信令。​

3.  B 接收好友申请，回复IM Server，IM Server标记消息为已读，创建一个点对点会话，等待消息的拉取。​

4.  A给B发送消息，IM Server收到A的消息后存储到send_msg和recieve_msg表，B轮询拉取。​

5.  B如果有多个设备端，则独立进行消息拉取。​

6.  如果B的设备刚刚登陆，则去IM server获取离线数据，客户端维护一个seq_id，IM Server只会返回大于此 seq_id的msg。​

7.  B客户端上报B用户已经读取的消息，IM Server更新flag标记为已读。​

8.  B从IM Server中拉取已读用户的列表，写一行sql select to from recieve_msg where msg_id=1 and flag=1 。​

## 更优解​

### 高性能​

#### 接入层优化​

1.  轮询拉模式，无法满足实时性要求，消息不能及时触达用户​

	1.  使用TCP长链接push消息给用户​

2.  客户端如何与服务端建立长链接？​

	1.  客户端只能通过公网IP，Socket TCP编程直接连接服务端​
	
	2.  通过一个IPConf服务下发公网IP给到客户端，灵活扩展并可智能调度长连接​
	
	3.  建立长连接后 IM Server 业务层拿到FD与uid的映射​
	
	4.  IPconf 通过协调服务与IM Server做服务发现，根据机器的负载状态进行负载均​

3.  长连接服务占用大量内存资源，IO密集度高，而IM Server操作数据库逻辑较重整体性能受限​

	1.  分离接入层与IM Server 业务层，长链接负责维护状态收发消息，IM Server负责业务​

4.  长连接服务为维持最优的消息通路，常需要以连接维度存储大量状态信息，用于做各种调度策略优化网络通信，同时会频繁迭代一些消息信令支持各种业务开发，这将导致由于服务的频繁迭代，对于一个有状态服务重启是缓慢的，并需要大量的重建连接影响消息收发延迟，该怎么办？​

	1.  将变与不变的业务逻辑在服务上做到物理隔离，变化的服务底层使用DB，使其做到无状态​
	
	2.  不变的消息通路(长连接网关) 保证其不会频繁迭代​
	
	3.  长连接服务同步更新state server, IpConf 服务读取状态信息用于旁路调度决策​
	
	4.  State 通过状态变化控制长连接的断开，通过MQ发送close信令用于长连接的调度 ​
	
	5.  如何知道客户端B的长连接在哪个接入层服务器上，进行消息路由并保证消息可靠送达呢？​

方案​|特点|优点​|缺点
---|---|---|---
广播|1.  IM server 将消息扇出式的发送给所有接入层服务​<br>2.  接入层服务只会处理自己持有uid的连接，而忽略其他​|1.  实现简单<br>​2.  适合超大聊天室场景​|1.  消息风暴，造成系统瘫痪<br>​2.  单聊场景过多的无效通信​
一致性哈希​|1.  ipConf服务与IM Server使用同一种hash方式​<br>2.  按uid进行分片，IM Server直接计算出该用户连接在哪个服务器上<br>​3.  通过统一的服务注册发现系统<br>​4.  当水平扩展接入层服务时，利用虚拟节点特性迁移长链接<br>​5.  对于迁移的用户来说，通过断线重连的方法实现​|1.  计算简单，消息路由没有性能损耗​<br>2.  实现简单​|1.  重度依赖服务发现系统的稳定性​<br>2.  水平扩展时需要进行连接迁移​<br>3.  受一致性hash算法的均匀性限制​
路由服务层​|1.  构建一个服务，底层使用kv存储uid与接入机器之间的映射关系<br>2.  同时可根据不同的会话类型组织映射关系，群组，多设备等kv关系​<br>3.  有接入层服务在线同步的方式更新kv<br>​4.  路由服务与IM Server之间维护消息队列，IM Server随意给到消息队<br>​5.  任意路由服务消费MQ解析此消息的路由信息确定接入层机器位置<br>​6.  根据接入层机器标识创建routeKey，将消息发送给此接入层机器专用队列​|1.  但可靠性高<br>​2.  通过MQ进行解耦，并很好的应对流量峰值<br>​3.  MQ保证消息可靠送达<br>​4.  路由服务无状态可水平扩展​|1.  实现较为复杂<br>​2.  需要独立维护路由层集群<br>​3.  服务依赖底层KV和MQ的稳定性​

​![[image (1).png]]
​

#### 存储层优化​

6.  读写比 1:9999,超大群每人发送一条消息要1亿分发量，应该如何控制超大群将机器资源耗尽？​

	1.  并发控制，做到按群维度限制最大并发数，保证超大群收发消息不至于导致系统崩溃​

7.  限制并发数量后必然会导致消息挤压，应该如何处理收发消息延迟增大的问题？​

	1.  达到并发限制后，消息如果积压则将一段窗口内的消息合并压缩后下发​
	
	2.  使用推拉结合的方式本质上就是通过信令对消息进行了压缩，但是多了一步拉取的网络调用​
	
	3.  如果用推拉结合的模式可以在一段时间窗口内聚合对同一用户的拉取信令减少网络调用次数​

8.  存储系统上存在的严重的写放大(扩散)问题？如何减少写入数据量，降低存储成本？​

	1.  对超大群降级为读放大模式进行存储，消息仅同步写入收件箱​
	
	2.  群状态信息，如回执消息等写入Hbase等存储中​

9.  那么回执消息应该如何处理？如何保证万人群中的已读/未读列表与数量的一致性?​

	1.  实时流处理，对于接收者已读消息的下发要同步进行，而接收者的消息已读可以异步落库​
	
	2.  对于超大群，群状态变更服务进行降级，不再发送信令通知(可自动识别群的聊天热度)​
	
	3.  异步写入通过重试保证最终一致性​

10.  从DB直接取数无法满足延迟10ms以下，应该如何优化？​

[各存储介质的读写耗时](https://zhuanlan.zhihu.com/p/99837672)​

​

方案​|特点|优点​|缺点​
---|---|---|---
分级存储​|1.  按uid维度在memDB上维护可排序的list，可根据活跃程度调节最大条数​<br>2.  对于超大群聊退化为读扩散模式，维护会话级别的List并异步缓存消息状态​<br>3.  按群聊的会话ID对消息列表进行本地LRU缓存，对列表的大小进行限制​<br>4.  使用protobuf进行序列化压缩处理，减少存储空​<br>5.  本地缓存的LRU使用HotRing算法，测算热点​<br>6.  使用消息队列广播消息状态的变化给到本地缓存，或者使用Gossip算法​<br>7.  限制拉取会话消息的切片范围，最多到memDB级别。​<br>8.  对于超过一定时间的数据压缩存储在文件系统中，提供OLAP查询​|1.  在线请求流量在MemDB终止​<br>2.  对请求按读取热度分级处理​<br>3.  对离线消息同步等range操作支持良好​<br>4.  可较为简单的支持群聊的推拉结合方案​|1.  IM Server的本地缓存预热较慢，服务重启期间会有所延<br>2.  大量使用内存，导致运维难度提升​<br>3.  消息的已读/未读状态的变更要维护多个缓存之间的一致性​<br>4.  要关注缓存命中率，缓存穿透等问题​
多元DB存储​|1.  使用rocksDB等kv引擎存储离线消息拉链，key 为会话ID value 是序列化消息列表​<br>2.  如果消息列表过长则对列表进行分列存储，key为会话ID，value为meta 索引信息​<br>3.  按会话ID+分段seqID作为key，value存储消息列表，读取时做合并​<br>4.  支持对一个key的value append 新的字节数据，支持列表的扩展与更新​<br>5.  对超过一定期限的消息存储在文件系统中，用于OLAP​<br>6.  使用消息ID作为key，用户id组成的列表作为value，存储消息已读/未读拉链​<br>7.  使用[NVMe SSD](https://zh.wikipedia.org/wiki/NVM_Express)等新硬件特性加速磁盘IO​|1.  使用多种NoSQL数据库，利用不同DB的优势补长取短​<br>2.  基于磁盘的KV存储解决了存储容量问题，便于运维​<br>3.  图数据库的使用，使其能够有更多策略上的优化​|1.  RocksDB 是单机数据库，需要自研开发分布式代理层​<br>2.  磁盘kv毕竟读写磁盘，其拉取消息列表的性能将受到影响，但吞吐却有所提升​<br>3.  rocksDB需要列表存储到达上限后需要自行实现逻辑实现对消息的更新，实现复杂​
图数据库​|1.  字节跳动分布式图数据库存储实战​<br>2.  ​[Dgraph: 同步复制,事务,分布式图数据库](https://hardcore.feishu.cn/docs/doccnE2vGujgtt4sID4gWLiMRbh)​​<br>3.  用图数据库存储好友关系，会话消息列表等多种拉链消息​<br>4.  图数据库存储用户关系，与群组关系，可以运行实时的近线OLAP查询快速识别热点群聊等极具价值的信息，利用这些信息的近实时的快速更新，可以快速识别热点对热点群聊进行特殊处理，比如增加消息列表存储上限，对热点群聊的会话消息进行本地缓存等，能够更加准确的预测热点提高命中率​|1.  低延迟，提供丰富灵活的查询功能​<br>2.  可以挖掘极具价值的社交信息​<br>3.  存储层DB统一方便运维​|1.  技术生态不完善，没有大规模商用开源案例​<br>2.  图数据库较为新颖，运行大规模图数据库运维成本高，出错后难以快速排查​<br>3.  图数据库提供了更多的功能，也必然导致其性能不如纯粹的KV引擎​
存储层代理服务​|1.  增加存储代理服务层，用来屏蔽底层存储细节，使得IM Server无感知​<br>2.  代理层基于key做hash分片，底层的kvserver 基于一致性协议进行复制，代理层进行分片与合并​<br>3.  对超大群聊来说，多个请求会同时拉取一份会话的消息列表，可以在代理层做一个自旋Cache，等待消息列表被缓存后在访问，减少下游DB的重复工作负载​|1.  业务隔离，对底层存储细节屏蔽​<br>2.  代理层无状态可水平扩展​<br>3.  可以做各种策略，优化查询​<br>4.  消息列表缓存在代理层，减少IM Server的内存负担，其重启过程系统会更加稳定​|1.  增加了一层逻辑，复杂度增大​

![[image (2).png]]
### 消息时序一致性​

[消息时序一致性](http://www.52im.net/thread-3189-1-1.html)​

1.  消息如何不丢失？​

	1.  上行消息客户端重试​
	
	2.  下行消息服务端重试​
	
	3.  接收消息方需要回复ACK，避免无限重试​

2.  如何防止重复？​

	1.  使用UUID去重复是否可以行？​
	
	2.  上行消息​

		1.  客户端在一个会话内生成一个消息的递增cid，服务器存储当前会话接收到的max cid​
		
		2.  如果发送的消息不是max_cid+1，则将其丢弃​

	3.  下行消息​

		1.  服务端为每个发送的消息分配seqid，客户的维护接收到的最大max_seq_id​
		
		2.  如果发送的消息不是max_seq_id+1，则将其丢弃​
	
3.  如何保证消息是有序的？​

	1.  按某个递增的消息ID排序​

		1.  上行消息按cid为每个消息分配seqID​
		
		2.  下行消息客户端按seqID进行排序​

4.  如何生成递增的消息ID？​

[解密融云IM产品的聊天消息ID生成策略](http://www.52im.net/thread-2747-1-1.html)​

[微信的海量IM聊天消息序列号生成实践(算法原理篇)](http://www.52im.net/thread-1998-1-1.html)​

​

方案​|特点|优点​|缺点​
---|---|---|---
纯拉模式|不存在一致性问题，只要保证上行消息一致性即可|实现是最为简单|实时性差​
单调递增ID生成器​|保证会话内seqID的‘万有一失’的单调递增性+拉模式消息补洞​|整体实现简单，id生成策略简单​|1.  双方通信次数过多，群聊性能差​<br>2.  依赖分布式ID生成系统的可靠性​
双ID链方法:​[Raft -论文导读 与 ETCD源码解读](https://hardcore.feishu.cn/docs/doccnMRVFcMWn1zsEYBrbsDf8De)|1.  接收方为每个会话保留preID(上一次接收到消息的id)​<br>2.  发送方发送当前消息的id以及上一次发送消息的id(preID)​<br>3.  形成消息链条，接收方对比preID判断是否一致来识别是否存在消息漏洞​<br>4.  如果preID不匹配，则将接收方的preID返回​<br>5.  发送方回退到接收方的preID的位置，重置所有消息心跳后重新发送​|不依赖id生成的单调性​|1.  双方通信次数过多，群聊性能差​<br>2.  下行消息实现机制复杂​
推拉结合|1.  下行消息给客户端发送拉取信令，通知客户端根据自身seqID拉取消息​<br>2.  pull 请求本身可作为上一次请求的ACK​|1.  下行消息不需要ack机制，服务端不需要维护超时重发逻辑​<br>2.  可通过一次拉取所有会话消息列表，减少网络调用次数​<br>3.  批量拉取利于对消息进行压缩，提高带宽利用率​|上行消息难以彻底解决时序性问题​
上行消息 PreID+下行消息 推拉结合​||1.  上行保证消息时序一致性​<br>2.  下行保证高吞吐​|上行消息在频发发送消息的场景下容易造成网络用塞​

​![[image (3).png]]

### 高可用​

1.  跨越公网的长连接如果断了怎么办？​

​![[Pasted image 20230313122846.png]]


	1.  心跳保活 ​
	
		1.  为什么应用层还需要心跳? tcp的心跳不好用吗？​
		
		2.  客户端周期性发送心跳给IM Gateway，IM Gateway重置内部定时器​
		
		3.  控制心跳包不要过大，通常在0.5kb以下​
		
		4.  心跳过长，服务端感知到已经断线的客户端的效率越低，资源利用率也就越低​
		
		5.  心跳过短，则会造成心跳潮汐，给网关造成流量压力(通过随机打撒来解决问题)​
		
		6.  发送心跳会经过很多运营商网络，不同地区运营商对于长链接的资源回收策略不同GGSN​
		
		7.  自适应心跳包(前台状态固定心跳，后台状态自适应:[测算NAT淘汰时间](https://cloud.tencent.com/developer/article/1030660))​
	
	2.  断线重连​
	
		1.  客户端由于网络原因断线(频繁切换网络，做地铁)，如何能快速稳定的重新建立长链接？​
		
			1.  断线后快速重连几次到原来的服务端​
			
			2.  服务端发现连接断开后不会立即删除用户的状态信息，等待一个超时时间​
			
			3.  在这段时间内，客户端快速建立新连接，服务端复用原有状态信息实现重连接​
		
		2.  服务端如果崩溃导致大规模客户端重新连接造成雪崩如何处理？​
		
			1.  IPConf服务通过服务发现机制快速识别服务端节点故障，进行调度​
			
			2.  客户端断线后，通过一定的随机策略打散重连请求的时间​
			
			3.  Get ipconf服务拿到新的ip调度列表重新调度，如果原有服务器还在其中则优先选​
	
	3.  心跳风暴​
	
		1.  心跳/消息超时计时器的数量与连接数和push消息数成正比，如果长链接服务崩溃未送达的消息如何再次重发？​
		
			1.  连接建立后需要调用一次离线消息同步接口，主动拉取消息，避免网关侧的状态丢失​
			
			2.  将连接的状态信息全部维护在state server中，与IM Gateway可以使用RPC或共享内存​
			
			3.  State server可以做一定的持久化处理，比喻使用快照&checkpoint机制​
			
			4.  但恢复的状态只能是其中一部分，与计时器相关的状态信息由于时效性无法恢复​
		
		2.  大量的定时器占用大量内存资源并且定时任务的触发会使得整个系统卡顿造成消息超时​
		
			1.  传统的计时器采用二叉堆来实现，存取定时任务的时间复杂度为LogN​
			
			2.  大量的消息收发会造成定时任务的频繁插入与删除，因此性能的瓶颈就在于数据结构​
			
			3.  使用[时间轮](https://blog.csdn.net/u013256816/article/details/80697456)算法可以常量级别的进行任务的插入与删除，但定时精度有所缺失​

2.  弱网场景下如何保证消息可靠触达?​

	1.  快链路 优化TCP连接​
	
		1.  限定传输数据包的大小1400字节，避免超过MSS造成IP数据分片​
		
		2.  放大TCP拥塞控制窗口，为有线网络设计的拥塞控制算法，并不适用于无线网络​
		
		3.  调整socket读写缓冲区，避免数据包溢出​
		
		4.  调整RTO初始值设置为3s，避免重试造成的堵塞​
		
		5.  禁用Nagle算法延迟算法，避免小数据包被协议栈缓存​
	
	2.  调度策略优化​
	
		1.  赛马机制，多个ip后台测速选择最快连接线路(服务端要对请求进行识别，避免资源浪费)​
		
		2.  调度策略基于客户端上报数据进行计算，ipconf分发最佳ip进行调度​
		
		3.  基于网络环境设置不同的超时时间，超时参数由Ipconf服务动态下发，加入策略计算​
		
		4.  短链退化， 当长链接断线次数太多，重连过于频繁的极端弱网环境例如地铁，则退化为轮询模式收发消息​
	
	3.  协议优化​
	
		1.  弱网场景下切换[QUIC协议](http://www.52im.net/thread-2816-1-1.html)​
		
		2.  精简协议包，采用自定义二进制协议​

3.  异地多活方案如何设计？​

​![[Pasted image 20230313123046.png]]

![](blob:https://bytedancecampus1.feishu.cn/a2089fbb-6d73-4cef-a64f-21c5d5f98e0e)

1.  核心思想：尽量减少广域请求​

2.  基于跨数据中心的[服务发现](https://deepzz.com/post/the-consul-of-discovery-and-configure-services.html)系统​

3.  通过IPconf进行旁路调度，IPConf跨数据中心感知到IM Gateway​

4.  IPConf基于客户端ip位置信息进行IDC的流量分片​

5.  基于位置策略调度最优IDC中的最优IM Gateway Server​

6.  如果IDC 不可用，IPConf服务自动切流量到可用的IDC上​

7.  每个IDC存储全量数据以应对机房级故障，基于底层同步组件同步数据​

8.  IM Server 通过跨IDC 的Route Server服务发现IM Gateway并通过跨IDC的MQ发送消息​

9.  Route Server 将自己IDC创建的kv对广播给其他IDC，以便于进行路由转发​

10.  IM Server转发消息给到接收者所在的IDC的IM Gateway上，消息内携带此消息写入的IDC​

11.  接收者根据push通知中携带的多个IDC信息，并行的去多个IDC拉取消息​

12.  为应对转发失败的情况，客户端可以在未收到任何push通知的一段时间后主动pull消息​

### 可运维性​

1.  如何做到良好的可观测性？​

	1.  展示能够暴露问题的指标​
	
	2.  搭建云原生架构​
	
	3.  构建指标观察平台​
	
		1.  业务指标 ​
		
			1.  识别热点情况，top活跃群聊/会话，热key等​
			
			2.  最小单元的QPS，延迟，失败率三大指标​
			
			3.  分布式链路跟踪​
	
	2.  系统指标​
	
		1.  应用服务监控​
		
		2.  数据中心/集群监控​
		
		3.  Docker 系统监控​
		
		4.  物理机系 统监控​
	
	3.  运行日志​
	
		1.  链路跟踪日志​
		
		2.  关键词检索日志​
		
		3.  异常，错误日志​

2.  如果减少人工运维的干预？并在紧急情况下可人工快速介入？​

	1.  接入服务网格，进行流量治理​
	
	2.  超时/重试/鉴权/限流/断流/风控 等功能平台化​
	
	3.  提供热备集群​

3.  部署，重启，修改，异常处理如何能影响更少的用户？​

	1.  实现长连接服务的[热重启](http://www.hitzhangjie.pro/blog/2020-08-28-go%E7%A8%8B%E5%BA%8F%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%83%AD%E9%87%8D%E5%90%AF/) | [MOSN 源码解析 - 启动流程](https://mosn.io/blog/code/mosn-startup/)|[MOSN 平滑升级原理](https://mosn.io/docs/concept/smooth-upgrade/)​
	
	2.  严格的代码发布流程，review机制等等​

​
![[image (4).png]]
![](blob:https://bytedancecampus1.feishu.cn/7704483a-75cf-4c67-8675-40afc95e16bb)

​

​

## 扩展问题​

### 安全性问题​

1.  端到端加密，作为IM系统用户的聊天数据均是私密信息，在现有的协议上对用户的聊天内容进行加密存储，并在另一端用户的客户端进行解密，是一种最理想的状态。​

### 多媒体消息​

1.  图片/视频/媒体上传 支持断点续传等功能，定义上传信令，通过局部敏感哈希识别重复数据。​

2.  表情包管理 对表情包进行编码，信令交换编码​

## 总结​

​![[Pasted image 20230313123242.png]]

![](blob:https://bytedancecampus1.feishu.cn/c25a05e5-8f8b-4742-af4b-93fdd9824160)

​

## 参考资料​

1.  [腾讯QQ1.4亿在线用户的技术挑战和架构演进之路](http://www.52im.net/thread-158-1-1.html)​

2.  [融云技术分享：全面揭秘亿级IM消息的可靠投递机制](http://www.52im.net/thread-3638-1-1.html)​

3.  [企业微信的IM架构设计揭秘:消息模型、万人群、已读回执、消息撤回](http://www.52im.net/thread-3631-1-1.html)​

4.  [消息表设计](https://www.codedump.info/post/20190608-im-msg-storage/)​

5.  [现代IM系统中的消息系统架构——架构篇-InfoQ](https://www.infoq.cn/article/yPB3Y2lv-DsFtRr5Cguv)​

6.  [如何设计一个亿级消息量的IM系统](https://xie.infoq.cn/article/19e95a78e2f5389588debfb1c)​

7.  [阿里钉钉技术分享:企业级IM王者--钉钉在后端架构上的过人之处](http://www.52im.net/thread-2848-1-1.html)​

8.  [IM架构优化策略汇总](https://cloud.tencent.com/developer/article/1553204)​

9.  [移动端IM开发者必读(二):史上最全移动弱网络优化方法总结](http://www.52im.net/thread-1588-1-1.html)​

10.  [弱网优化方法](https://cloud.tencent.com/developer/article/1005365)​

11.  [融云首席架构师李淼:即时通讯服务异地双活的那些事儿](http://www.52im.net/article-98-1.html)​

12.  [淘宝技术分享:手淘亿级移动端接入层网关的技术演进之路](http://www.52im.net/thread-3110-1-1.html)​

13.  [谷歌SRE与运维工作的思考](https://blog.csdn.net/u013256816/article/details/104177489)